in the CDC take care of insert update and delete 

meta data driven architecture
streamlit run app.py --server.port 8502




CDC Deletion and updation was done 

but in the updation when we update the sales data how to sales_data will take it because the key will become duplicate

delete the row first if it is exist then insert 




product_dim 
product_id, product_name, timestamp

state_dim 
state_id, state_name, state_code, time_stamp

city_dim 
city_id, city_name, is_capital, time_stamp

branch_dim 
branch_id, branch_name, is_main, timestamp


shopping_dim 
experience_id shopping experience 

payment_method_dim
payment id 
payment method 
timestamp


sales_fact 

id, product_id from product_dim , quantity, state_id from state_dim, city_id from city_dim, branch_id from city_dim, shopping experience from shopping_dim, payment method from payment_dim, total amount, timestamp

sales_data 

uuid, state, branch, product name, text, sentiment_result, sales_id 





sales_fact 1:1 product_dim 

sales_fact 1:1 


there will be no product_id in the kafka topic and in the spark dataframe